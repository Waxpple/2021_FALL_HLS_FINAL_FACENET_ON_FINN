{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HLS_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hxqoz9U712Dc",
        "outputId": "2f62a541-b547-4818-d14c-89899917a893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.7/dist-packages (from -r /content/HLSFinal/requirement.txt (line 1)) (2.5.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r /content/HLSFinal/requirement.txt (line 2)) (4.62.3)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (from -r /content/HLSFinal/requirement.txt (line 3)) (0.1.12)\n",
            "Requirement already satisfied: brevitas in /usr/local/lib/python3.7/dist-packages (from -r /content/HLSFinal/requirement.txt (line 4)) (0.7.1)\n",
            "Requirement already satisfied: onnxoptimizer in /usr/local/lib/python3.7/dist-packages (from -r /content/HLSFinal/requirement.txt (line 5)) (0.2.6)\n",
            "Requirement already satisfied: protobuf==3.19.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/HLSFinal/requirement.txt (line 6)) (3.19.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch->-r /content/HLSFinal/requirement.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch->-r /content/HLSFinal/requirement.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch->-r /content/HLSFinal/requirement.txt (line 1)) (0.11.1+cu111)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch->-r /content/HLSFinal/requirement.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: imgaug<0.2.7,>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from albumentations->-r /content/HLSFinal/requirement.txt (line 3)) (0.2.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from albumentations->-r /content/HLSFinal/requirement.txt (line 3)) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations->-r /content/HLSFinal/requirement.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations->-r /content/HLSFinal/requirement.txt (line 3)) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations->-r /content/HLSFinal/requirement.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/HLSFinal/requirement.txt (line 3)) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/HLSFinal/requirement.txt (line 3)) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/HLSFinal/requirement.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/HLSFinal/requirement.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/HLSFinal/requirement.txt (line 3)) (2021.11.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/HLSFinal/requirement.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/HLSFinal/requirement.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/HLSFinal/requirement.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/HLSFinal/requirement.txt (line 3)) (3.0.6)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from brevitas->-r /content/HLSFinal/requirement.txt (line 4)) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from brevitas->-r /content/HLSFinal/requirement.txt (line 4)) (3.10.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from brevitas->-r /content/HLSFinal/requirement.txt (line 4)) (21.3)\n",
            "Requirement already satisfied: dependencies==2.0.1 in /usr/local/lib/python3.7/dist-packages (from brevitas->-r /content/HLSFinal/requirement.txt (line 4)) (2.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from brevitas->-r /content/HLSFinal/requirement.txt (line 4)) (57.4.0)\n",
            "Requirement already satisfied: future-annotations in /usr/local/lib/python3.7/dist-packages (from brevitas->-r /content/HLSFinal/requirement.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.7/dist-packages (from onnxoptimizer->-r /content/HLSFinal/requirement.txt (line 5)) (1.10.2)\n",
            "Requirement already satisfied: tokenize-rt>=3 in /usr/local/lib/python3.7/dist-packages (from future-annotations->brevitas->-r /content/HLSFinal/requirement.txt (line 4)) (4.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch->-r /content/HLSFinal/requirement.txt (line 1)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch->-r /content/HLSFinal/requirement.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch->-r /content/HLSFinal/requirement.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch->-r /content/HLSFinal/requirement.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "#!git clone https://github.com/darrenyaoyao/HLSFinal.git\n",
        "!pip3 install -r /content/HLSFinal/requirement.txt\n",
        "!pip3 install joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/HLSFinal\")"
      ],
      "metadata": {
        "id": "77-Lxg9N2wJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "\n",
        "def get_files(path='./', ext=('.png', '.jpeg', '.jpg')):\n",
        "    \"\"\" Get all image files \"\"\"\n",
        "    files = []\n",
        "    for e in ext:\n",
        "        files.extend(glob.glob(f'{path}/**/*{e}'))\n",
        "    files.sort(key=lambda p: (os.path.dirname(p), int(os.path.basename(p).split('.')[0])))\n",
        "    return files\n",
        "\n",
        "def to_rgb_and_save(path):\n",
        "    \"\"\" Some of the images may have RGBA mode \"\"\"\n",
        "    for p in path:\n",
        "        img = Image.open(p)\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "            img.save(p)\n",
        "\n",
        "ABS_PATH = './'\n",
        "DATA_PATH = os.path.join(ABS_PATH, 'data')\n",
        "\n",
        "TRAIN_DIR = os.path.join(DATA_PATH, 'train_images')\n",
        "TEST_DIR = os.path.join(DATA_PATH, 'test_images')\n",
        "\n",
        "ALIGNED_TRAIN_DIR = TRAIN_DIR + '_cropped'\n",
        "ALIGNED_TEST_DIR = TEST_DIR + '_cropped'"
      ],
      "metadata": {
        "id": "2E0dBkCQ3Gh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "# 1. Get path for TRAIN_DIR/TEST_DIR\n",
        "trainF, testF = get_files(TRAIN_DIR), get_files(TEST_DIR)\n",
        "\n",
        "# prepare info for printing\n",
        "trainC, testC = Counter(map(os.path.dirname, trainF)), Counter(map(os.path.dirname, testF))\n",
        "train_total, train_text  = sum(trainC.values()), '\\n'.join([f'\\t- {os.path.basename(fp)} - {c}' for fp, c in trainC.items()])\n",
        "test_total, test_text  = sum(testC.values()), '\\n'.join([f'\\t- {os.path.basename(fp)} - {c}' for fp, c in testC.items()])\n",
        "\n",
        "print(f'Train files\\n\\tpath: {TRAIN_DIR}\\n\\ttotal number: {train_total}\\n{train_text}')\n",
        "print(f'Train files\\n\\tpath: {TEST_DIR}\\n\\ttotal number: {test_total}\\n{test_text}')\n",
        "\n",
        "to_rgb_and_save(trainF), to_rgb_and_save(testF)"
      ],
      "metadata": {
        "id": "vqZbEeVa3JY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1, training, fixed_image_standardization\n",
        "import tqdm\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import shutil\n",
        "from brevitas.quant_tensor import QuantTensor\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Running on device: {device}')\n",
        "\n",
        "\n",
        "def crop_face_and_save(path, new_path=None, model=MTCNN, transformer=None, params=None):\n",
        "    \"\"\"\n",
        "    Detect face on each image, crop them and save to \"new_path\"\n",
        "    :param str path: path with images will be passed to  datasets.ImageFolder\n",
        "    :param str new_path: path to locate new \"aligned\" images, if new_path is None\n",
        "                     then new_path will be path + \"_cropped\"\n",
        "    :param model: model to detect faces, default MTCNN\n",
        "    :param transformer: transformer object will be passed to ImageFolder\n",
        "    :param params: parameters of MTCNN model\n",
        "    \"\"\"\n",
        "    if not new_path:\n",
        "        new_path = path + '_cropped'\n",
        "\n",
        "    # in case new_path exists MTCNN model will raise error\n",
        "    if os.path.exists(new_path):\n",
        "        shutil.rmtree(new_path)\n",
        "\n",
        "    # it is default parameters for MTCNN\n",
        "    if not params:\n",
        "        params = {\n",
        "            'image_size': 160, 'margin': 0,\n",
        "            'min_face_size': 10, 'thresholds': [0.6, 0.7, 0.7],\n",
        "            'factor': 0.709, 'post_process': False, 'device': device\n",
        "            }\n",
        "\n",
        "    model = model(**params)\n",
        "\n",
        "    if not transformer:\n",
        "        transformer = transforms.Lambda(\n",
        "            lambd=lambda x: x.resize((1280, 1280)) if (np.array(x) > 2000).all() else x\n",
        "        )\n",
        "    # for convenience we will use ImageFolder instead of getting Image objects by file paths\n",
        "    dataset = datasets.ImageFolder(path, transform=transformer)\n",
        "    dataset.samples = [(p, p.replace(path, new_path)) for p, _ in dataset.samples]\n",
        "\n",
        "    # batch size 1 as long as we havent exact image size and MTCNN will raise an error\n",
        "    loader = DataLoader(dataset, batch_size=1, collate_fn=training.collate_pil)\n",
        "    for i, (x, y) in enumerate(tqdm.tqdm(loader)):\n",
        "        model(x, save_path=y)\n",
        "\n",
        "    # spare some memory\n",
        "    del model, loader, dataset\n",
        "\n",
        "# 3. Crop train dataset faces and save aligned images\n",
        "print('\\t- Train data')\n",
        "crop_face_and_save(TRAIN_DIR, ALIGNED_TRAIN_DIR)\n",
        "\n",
        "# check if some imgs were missed by detector and failed to save\n",
        "train_files, train_aligned_files = get_files(TRAIN_DIR), get_files(ALIGNED_TRAIN_DIR)\n",
        "if len(train_files) != len(train_aligned_files):\n",
        "    files = set(map(lambda fp: os.path.relpath(fp, start=TRAIN_DIR), train_files))\n",
        "    aligned_files = set(map(lambda fp: os.path.relpath(fp, start=ALIGNED_TRAIN_DIR), train_aligned_files))\n",
        "    detect_failed_train_files = list(files - aligned_files)\n",
        "    print(f\"\\nfiles {len(aligned_files)}/{len(files)}: {', '.join(detect_failed_train_files)} were not saved\")\n",
        "\n",
        "# -------------                     -------------\n",
        "\n",
        "# Crop test dataset faces and save aligned images\n",
        "print('\\t- Test data')\n",
        "crop_face_and_save(TEST_DIR, ALIGNED_TEST_DIR)\n",
        "\n",
        "# check if some imgs were missed by detector and failed to save\n",
        "test_files, test_aligned_files = get_files(TEST_DIR), get_files(ALIGNED_TEST_DIR)\n",
        "if len(test_files) != len(test_aligned_files):\n",
        "    files = set(map(lambda fp: os.path.relpath(fp, start=TEST_DIR), test_files))\n",
        "    aligned_files = set(map(lambda fp: os.path.relpath(fp, start=ALIGNED_TEST_DIR), test_aligned_files))\n",
        "    detect_failed_test_files = list(files - aligned_files)\n",
        "    print(f\"\\nfiles {len(aligned_files)}/{len(files)}: {', '.join(detect_failed_train_files)} were not saved\")\n",
        "\n",
        "trainF = get_files(ALIGNED_TRAIN_DIR)\n",
        "testF = get_files(ALIGNED_TEST_DIR)\n",
        "\n",
        "import albumentations as A\n",
        "\n",
        "from facenet_pytorch import fixed_image_standardization\n",
        "\n",
        "standard_transform = transforms.Compose([\n",
        "                                np.float32,\n",
        "                                transforms.ToTensor(),\n",
        "                                fixed_image_standardization\n",
        "])\n",
        "\n",
        "aug_mask = A.Compose([\n",
        "                   A.HorizontalFlip(p=0.5),\n",
        "                   A.VerticalFlip(p=0.15),\n",
        "                   A.RandomContrast(limit=0.5, p=0.4),\n",
        "                   A.Rotate(30, p=0.2),\n",
        "                   A.RandomSizedCrop((120, 120), 160, 160, p=0.4),\n",
        "                   A.OneOrOther(A.JpegCompression(p=0.2), A.Blur(p=0.2), p=0.66),\n",
        "                   A.OneOf([\n",
        "                            A.Rotate(45, p=0.3),\n",
        "                            A.ElasticTransform(sigma=20, alpha_affine=20, border_mode=0, p=0.2)\n",
        "                            ], p=0.5),\n",
        "                  A.HueSaturationValue(val_shift_limit=10, p=0.3)\n",
        "            ], p=1)\n",
        "\n",
        "transform = {\n",
        "    'train': transforms.Compose([\n",
        "                      transforms.Lambda(lambd=lambda x: aug_mask(image=np.array(x))['image']),\n",
        "                      standard_transform\n",
        "                  ]),\n",
        "    'test': standard_transform\n",
        "}\n",
        "\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "\n",
        "\n",
        "b = 32\n",
        "\n",
        "# Original train images\n",
        "trainD = datasets.ImageFolder(ALIGNED_TRAIN_DIR, transform=standard_transform)\n",
        "# Augmented train images\n",
        "trainD_aug = datasets.ImageFolder(ALIGNED_TRAIN_DIR, transform=transform['train'])\n",
        "# Train Loader\n",
        "trainL = DataLoader(trainD, batch_size=b, num_workers=2) # x: torch.Size([batch_size, 3, 160, 160]), y: torch.Size([batch_size])\n",
        "trainL_aug = DataLoader(trainD_aug, batch_size=b, num_workers=2)\n",
        "\n",
        "# Original test images\n",
        "testD = datasets.ImageFolder(ALIGNED_TEST_DIR, transform=standard_transform)\n",
        "# Test Loader\n",
        "testL = DataLoader(testD, batch_size=b, num_workers=2)\n",
        "\n",
        "# Convert encoded labels to named claasses\n",
        "IDX_TO_CLASS = np.array(list(trainD.class_to_idx.keys()))\n",
        "CLASS_TO_IDX = dict(trainD.class_to_idx.items())\n",
        "\n",
        "from model import QuantWeightLeNet\n",
        "del model\n",
        "model = QuantWeightLeNet().to(device)\n",
        "\n",
        "def fixed_denormalize(image):\n",
        "    \"\"\" Restandartize images to [0, 255]\"\"\"\n",
        "    return image * 128 + 127.5\n",
        "\n",
        "from random import randrange\n",
        "\n",
        "criterion = torch.nn.TripletMarginLoss(margin=0.05, p=2).to(device=device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                lr=1e-5,\n",
        "                                weight_decay=0.9)\n",
        "\n",
        "# Training\n",
        "model\n",
        "for n_i in tqdm.trange(1000):\n",
        "    for i, (x, y) in enumerate(trainL, 1):\n",
        "\n",
        "        x = fixed_denormalize(x)\n",
        "        anchor_x = x[[0,2,4,6,8,10,12,14,16]]\n",
        "        anchor_y = y[[0,2,4,6,8,10,12,14,16]]\n",
        "        positive_x = x[[1,3,5,7,9,11,13,15,17]]\n",
        "        positive_y = y[[1,3,5,7,9,11,13,15,17]]\n",
        "\n",
        "        negative_index = [randrange(2,18),\n",
        "                          randrange(0,2) if randrange(0,2) else randrange(4, 18),\n",
        "                          randrange(0,4) if randrange(0,2) else randrange(6, 18),\n",
        "                          randrange(0,6) if randrange(0,2) else randrange(8, 18),\n",
        "                          randrange(0,8) if randrange(0,2) else randrange(10, 18),\n",
        "                          randrange(0,10) if randrange(0,2) else randrange(12, 18),\n",
        "                          randrange(0,12) if randrange(0,2) else randrange(14, 18),\n",
        "                          randrange(0,14) if randrange(0,2) else randrange(16, 18),\n",
        "                          randrange(0,16)]\n",
        "\n",
        "        negative_x = x[negative_index]\n",
        "        negative_y = y[negative_index]\n",
        "\n",
        "        anchor_embed = model(anchor_x.to(device))\n",
        "        positive_embed = model(positive_x.to(device))\n",
        "        negative_embed = model(negative_x.to(device))\n",
        "\n",
        "        loss = criterion(anchor_embed, positive_embed, negative_embed)\n",
        "        print(n_i, \": \", loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "9n1jtxhM1-G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import ceil \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "%matplotlib inline \n",
        "\n",
        "from matplotlib.patches import Ellipse\n",
        "\n",
        "\n",
        "def imshow(img, ax, title):  \n",
        "    ax.imshow(img)\n",
        "    if title:\n",
        "        el = Ellipse((2, -1), 0.5, 0.5)\n",
        "        ax.annotate(title, xy=(1, 0), xycoords='axes fraction', ha='right', va='bottom',\n",
        "                    bbox=dict(boxstyle=\"round\", fc=\"0.8\"), \n",
        "                    arrowprops=dict(arrowstyle=\"simple\", fc=\"0.6\", ec=\"none\", \n",
        "                                    patchB=el, connectionstyle=\"arc3, rad=0.3\"))\n",
        "    ax.set_xticks([]), ax.set_yticks([])\n",
        "\n",
        "def plot_gallery(images, ncols, nrows, titles=None, title='', figsize=None): \n",
        "    if figsize is None: \n",
        "        figsize = (18, ncols) if ncols < 10 else (18, 20)  \n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    grid = ImageGrid(fig, 111, nrows_ncols=(nrows, ncols), axes_pad=0.02)\n",
        "\n",
        "    for i, ax in enumerate(grid): \n",
        "        if i == len(images): break \n",
        "        imshow(images[i], ax, titles[i] if titles is not None else '')\n",
        "\n",
        "    # there are some problems with suptitle alignment \n",
        "    y_title_pos = grid[0].get_position().get_points()[1][1] - 0.33 / (1 if nrows == 1 else nrows / 3)\n",
        "    plt.suptitle(title, y=y_title_pos, fontsize=12)\n",
        "\n",
        "def plot(paths=None, images=None, titles=None, axtitle=True, title='', to_size=(512, 512)): \n",
        "    \"\"\"\n",
        "    Plot image gallery by passing (paths, title) or (images, titles)\n",
        "    :param paths: list of image paths\n",
        "    :param images: list of (PIL.Image | np.array | torch.Tensor) objects \n",
        "    :param titles: list of image titles \n",
        "    :param bool axtitle: if paths is not None, then axtitle=True leads to use basedir name as titles \n",
        "    :param str title: gallery title   \n",
        "    :param to_size: image resizing size before plot, default (512, 512)\n",
        "    \"\"\"\n",
        "\n",
        "    if paths is not None and len(paths): \n",
        "        images = [Image.open(p).resize(to_size) for p in paths]\n",
        "\n",
        "        nrows = int(ceil(len(images) / 12)) # 12 images per row \n",
        "        ncols = 12 if nrows > 1 else len(images)\n",
        "\n",
        "        if axtitle: \n",
        "              titles = [os.path.dirname(p).split('/')[-1] for p in paths]\n",
        "\n",
        "        plot_gallery(images, ncols, nrows, titles, title)\n",
        "\n",
        "    elif images is not None and len(images): \n",
        "        if isinstance(images, list): \n",
        "            images = np.array(images)\n",
        "\n",
        "        nrows = int(ceil(len(images) / 12)) # 12 images per row \n",
        "        ncols = 12 if nrows > 1 else len(images)\n",
        "\n",
        "        # Rescale to [0., 1.]\n",
        "        if images[0].max() > 1: \n",
        "            images /= 255. \n",
        "\n",
        "        # if torch.Tensor change axes \n",
        "        if not isinstance(images, np.ndarray): \n",
        "            if images.size(1) == 3 or 1: \n",
        "                images = images.permute((0, 2, 3, 1))\n",
        "\n",
        "        plot_gallery(images, ncols, nrows, titles, title)\n",
        "\n",
        "\n",
        "    else: \n",
        "        raise LookupError('You didnt pass any path or image objects')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kr-Tw6N54ey8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainF = get_files(ALIGNED_TRAIN_DIR)\n",
        "plot(paths=trainF, title='Aligned train images')"
      ],
      "metadata": {
        "id": "kEqAHorV4iZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fixed_denormalize(image): \n",
        "    \"\"\" Restandartize images to [0, 255]\"\"\"\n",
        "    return image * 128 + 127.5\n",
        "\n",
        "def getEmbeds(model, n, loader, imshow=False, n_img=5):\n",
        "    model.eval()\n",
        "    # images to display \n",
        "    images = []\n",
        "\n",
        "    embeds, labels = [], []\n",
        "    for n_i in tqdm.trange(n): \n",
        "        for i, (x, y) in enumerate(loader, 1): \n",
        "\n",
        "            # on each first batch get 'n_img' images  \n",
        "            if imshow and i == 1: \n",
        "                inds = np.random.choice(x.size(0), min(x.size(0), n_img))\n",
        "                images.append(fixed_denormalize(x[inds].data.cpu()).permute((0, 2, 3, 1)).numpy())\n",
        "\n",
        "            embed = model(x.to(device))\n",
        "            embed = embed.tensor.cpu().detach().numpy()\n",
        "            embeds.append(embed), labels.extend(y.data.cpu().numpy())\n",
        "\n",
        "    if imshow: \n",
        "        plot(images=np.concatenate(images))\n",
        "\n",
        "    return np.concatenate(embeds), np.array(labels)"
      ],
      "metadata": {
        "id": "RdFuz2rC5FAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.eval()\n",
        "# # images to display \n",
        "# images = []\n",
        "# loader = trainL\n",
        "# n_img=1\n",
        "# embeds, labels = [], []\n",
        "# for n_i in tqdm.trange(1): \n",
        "#     for i, (x, y) in enumerate(loader, 1): \n",
        "\n",
        "#         # on each first batch get 'n_img' images  \n",
        "#         if imshow and i == 1: \n",
        "#             inds = np.random.choice(x.size(0), min(x.size(0), n_img))\n",
        "#             images.append(fixed_denormalize(x[inds].data.cpu()).permute((0, 2, 3, 1)).numpy())\n",
        "\n",
        "#         embed = model(x.to(device))\n",
        "#         print(type(embed.tensor.cpu().detach().numpy()))"
      ],
      "metadata": {
        "id": "m9oRnfTO9cwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Get embeddings \n",
        "# Train embeddings \n",
        "trainEmbeds, trainLabels = getEmbeds(model.to(device), 1, trainL, False)\n",
        "trainEmbeds_aug, trainLabels_aug = getEmbeds(model.to(device), 50, trainL_aug, imshow=True, n_img=3)\n",
        "\n",
        "trainEmbeds = np.concatenate([trainEmbeds, trainEmbeds_aug])\n",
        "trainLabels = np.concatenate([trainLabels, trainLabels_aug])\n",
        "\n",
        "# Test embeddings \n",
        "testEmbeds, testLabels = getEmbeds(model, 1, testL, False)"
      ],
      "metadata": {
        "id": "X3F_6IiG56by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Save embeddings \n",
        "TRAIN_EMBEDS = os.path.join(DATA_PATH, 'trainEmbeds.npz')\n",
        "TEST_EMBEDS = os.path.join(DATA_PATH, 'testEmbeds.npz')\n",
        "\n",
        "np.savez(TRAIN_EMBEDS, x=trainEmbeds, y=trainLabels)\n",
        "np.savez(TEST_EMBEDS, x=testEmbeds, y=testLabels)\n",
        "\n",
        "# Load the saved embeddings to use them futher \n",
        "trainEmbeds, trainLabels = np.load(TRAIN_EMBEDS, allow_pickle=True).values()\n",
        "testEmbeds, testLabels = np.load(TEST_EMBEDS, allow_pickle=True).values()\n",
        "\n",
        "# Get named labels\n",
        "trainLabels, testLabels = IDX_TO_CLASS[trainLabels], IDX_TO_CLASS[testLabels]"
      ],
      "metadata": {
        "id": "S7rLEIRlEKmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import pairwise_distances\n",
        "import pandas as pd\n",
        "import seaborn as sns \n",
        "sns.set()\n",
        "\n",
        "def getDist(x, metric='euclidean', index=None, columns=None):\n",
        "    dists = pairwise_distances(x, x, metric=metric)\n",
        "    return pd.DataFrame(dists, index=index, columns=columns)\n",
        "\n",
        "def heatmap(x, title='', cmap='Greens', linewidth=1):\n",
        "    plt.figure(figsize=(17, 12))\n",
        "    plt.title(title)\n",
        "    sns.heatmap(x, cmap=cmap, square=True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4iy8NLzWEVgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note 88 first images are original and 4247 are augmented\n",
        "# as long as to calculate (4335, 512) distance matrix is time consuming we get only distances of originals \n",
        "inds = range(18)\n",
        "\n",
        "# Train embeddings \n",
        "dists = getDist(trainEmbeds[inds], metric='euclidean', index=trainLabels[inds], columns=trainLabels[inds])\n",
        "heatmap(dists, 'euclidean distance')\n",
        "\n",
        "dists = getDist(trainEmbeds[inds], metric='cosine', index=trainLabels[inds], columns=trainLabels[inds])\n",
        "heatmap(dists, 'cosine distance')"
      ],
      "metadata": {
        "id": "aM5Lhio6EYHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test embeddings \n",
        "dists = getDist(testEmbeds, metric='euclidean', index=testLabels, columns=testLabels)\n",
        "heatmap(dists, 'euclidean distance')\n",
        "\n",
        "dists = getDist(testEmbeds, metric='cosine', index=testLabels, columns=testLabels)\n",
        "heatmap(dists, 'cosine distance')"
      ],
      "metadata": {
        "id": "dw1gMPe3E1oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "\n",
        "inds = range(18)\n",
        "X_tsne1 = TSNE(n_components=2, init='pca', random_state=33).fit_transform(trainEmbeds[inds])\n",
        "X_tsne2 = TSNE(n_components=2, init='random', random_state=33).fit_transform(trainEmbeds[inds])\n",
        "y = [CLASS_TO_IDX[label] for label in trainLabels[inds]]\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 8))\n",
        "\n",
        "img = ax[0].scatter(X_tsne1[:, 0], X_tsne1[:, 1], c=y, alpha=0.5, cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
        "ax[1].scatter(X_tsne2[:, 0], X_tsne2[:, 1], c=y, alpha=0.5, cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
        "\n",
        "ax[0].set_title('TSNE with pca init')\n",
        "ax[1].set_title('TSNE with random init')\n",
        "plt.suptitle('Face embeddings')\n",
        "\n",
        "cbar = plt.colorbar(img, ax=ax)\n",
        "cbar.ax.set_yticklabels(np.unique(trainLabels[inds])) \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A9SN8nQ4FU4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "inds = range(18)\n",
        "print(type(inds))\n",
        "X_pca1 = PCA(n_components=2, random_state=33).fit_transform(trainEmbeds[inds])\n",
        "y = [CLASS_TO_IDX[label] for label in trainLabels[inds]]\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "img = ax.scatter(X_pca1[:, 0], X_pca1[:, 1], c=y, alpha=0.5, cmap=plt.cm.get_cmap('nipy_spectral', 9))\n",
        "\n",
        "plt.title('PCA method')\n",
        "plt.suptitle('Face embeddings')\n",
        "\n",
        "cbar = plt.colorbar(img, ax=ax)\n",
        "cbar.ax.set_yticklabels(np.unique(trainLabels[inds])) \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7tmg6IQuF_Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data preparation \n",
        "X = np.copy(trainEmbeds)\n",
        "y = np.array([CLASS_TO_IDX[label] for label in trainLabels])\n",
        "\n",
        "print(f'X train embeds size: {X.shape}')\n",
        "print(f'Tagret train size: {y.shape}')"
      ],
      "metadata": {
        "id": "X6nzH6ogGDV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore', 'Solver terminated early.*')\n",
        "\n",
        "param_grid = {'C': [1, 10, 100, 1e3, 5e3, 1e4, 5e4, 1e5],\n",
        "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1, 'auto'],\n",
        "              'kernel': ['rbf', 'sigmoid', 'poly']}\n",
        "model_params = {'class_weight': 'balanced', 'max_iter': 10, 'probability': True, 'random_state': 3}\n",
        "model = SVC(**model_params)\n",
        "clf = GridSearchCV(model, param_grid)\n",
        "clf.fit(X, y)\n",
        "\n",
        "print('Best estimator: ', clf.best_estimator_)\n",
        "print('Best params: ', clf.best_params_)"
      ],
      "metadata": {
        "id": "aqsddHdaGF3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "SVM_PATH = os.path.join(DATA_PATH, 'svm.sav')\n",
        "joblib.dump(clf, SVM_PATH)\n",
        "clf = joblib.load(SVM_PATH)"
      ],
      "metadata": {
        "id": "zuJwYFcSGHed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test data preparation \n",
        "X_test, y_test = np.copy(testEmbeds), np.array([CLASS_TO_IDX[label] for label in testLabels])\n",
        "print(f'X train embeds size: {X_test.shape}')\n",
        "print(f'Tagret train size: {y_test.shape}')"
      ],
      "metadata": {
        "id": "eRTVm_zBGIck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "inds = range(18)\n",
        "train_acc = accuracy_score(clf.predict(X[inds]), y[inds])\n",
        "print(f'Accuracy score on train data: {train_acc:.3f}')\n",
        "\n",
        "test_acc = accuracy_score(clf.predict(X_test), y_test)\n",
        "print(f'Accuracy score on test data: {test_acc}')"
      ],
      "metadata": {
        "id": "NQAjiS6CGJnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from brevitas.export import FINNManager\n",
        "\n",
        "FINNManager.export(model.to('cpu'), input_shape=(1, 3, 160, 160), export_path=\"facenet.onnx\")"
      ],
      "metadata": {
        "id": "mT8J4GK13XYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JnlP71t82RDH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}